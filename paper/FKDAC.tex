\documentclass{article}
\usepackage{amsmath,amssymb,graphicx,bbm}
\usepackage[english]{babel}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\assign}{:=}
\newcommand{\nobracket}{}
\newcommand{\nocomma}{}
\newcommand{\tmmathbf}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmtextbf}[1]{{\bfseries{#1}}}
\newcommand{\tmtextit}[1]{{\itshape{#1}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newenvironment{tmindent}{\begin{tmparmod}{1.5em}{0pt}{0pt} }{\end{tmparmod}}
\newenvironment{tmparmod}[3]{\begin{list}{}{\setlength{\topsep}{0pt}\setlength{\leftmargin}{#1}\setlength{\rightmargin}{#2}\setlength{\parindent}{#3}\setlength{\listparindent}{\parindent}\setlength{\itemindent}{\parindent}\setlength{\parsep}{\parskip}} \item[]}{\end{list}}
\newenvironment{tmparsep}[1]{\begingroup\setlength{\parskip}{#1}}{\endgroup}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Fast KDAC}
\author{}
\maketitle

Given the objective function :
\begin{equation}
  \begin{array}{ll}
    \min & - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{tr} ( W^T A_{i, j}
    W)}{2 \sigma^2}}\\
    W & \\
    s.t & W^T W = I\\
    & W \in \mathbbm{R}^{d \times q}\\
    & A \in \mathbbm{R}^{d \times d}\\
    & \gamma_{i, j} \in \mathbbm{R}
  \end{array}
\end{equation}
To optimize this cost function, the original KDAC uses an optimization
technique called Dimensional Growth (DG). It rewrites the cost function into
separate columns of the $W$ matrix, and solve the problem one column at a time
in a Greedy fashion.
\[ \begin{array}{llllll}
     \min & - \sum_{i, j} \gamma_{i, j} e^{- \frac{w_1^T A_{i, j} w_1}{2
     \sigma^2}} e^{- \frac{w_2^T A_{i, j} w_2}{2 \sigma^2}} \ldots e^{-
     \frac{w_q^T A_{i, j} w_q}{2 \sigma^2}} &  &  &  & w_i = i \tmop{th}
     \tmop{column} \tmop{of} W\\
     W &  &  &  &  & \\
     s.t & W^T W = I &  &  &  & 
   \end{array} \]
For example, to solve the first column $w_1$, it ignores the rest of the
columns and simplify the problem into :
\[ \begin{array}{ll}
     \min & - \sum_{i, j} \gamma_{i, j} e^{- \frac{w_1^T A_{i, j} w_1}{2
     \sigma^2}}\\
     w_1 & \\
     s.t & w_1^T w_1 = 1
   \end{array} \]
This problem could be solved using standard Gradient methods. Once $w_1$ is
computed, DG then treats the exponential term as a constant $g ( w_1)$ and
solve for the next variable.
\[ \begin{array}{ll}
     f ( w_2) = & - \sum_{i, j} \gamma_{i, j} e^{- \frac{w_1^T A_{i, j} w_1}{2
     \sigma^2}} e^{- \frac{w_2^T A_{i, j} w_2}{2 \sigma^2}}
   \end{array} \]
\[ \begin{array}{ll}
     f ( w_2) = & - \sum_{i, j} \gamma_{i, j} g ( w_1) e^{- \frac{w_2^T A_{i,
     j} w_2}{2 \sigma^2}}
   \end{array} \]
Without the orthgonality constraint, each stage of the optimization process
could be solved using Gradient methods. However, the orthogonality constraint
of $W^T W = I$ requires further complication to ensure compliance. To start,
the initialization of each new column $w_i$ must go through the Gram Schmit
method to ensure its orthogonality against all previous columns. Further more,
the gradient direction calculated during each iterations also must undergo
Gran Schmit. By removing components from previous vectors, it ensures that
each update of $w_i$ maintains feasibility.



Dimension Growth was the original approach used to solve the optimization
problem of equation (1), and it achieved its objective of demonstrating the
viability of KDAC. However, as the technology approach its next developmental
stage, the implementation of this technology on large scale data requires KDAC
to adapt for a more implementable algorithm.



The complexity of Dimension Growth algorithm heavily increases time of code
development. This issue is especially prominent when speed requirement forces
the development to be done in C or on the GPU. In these cases, simpler
algorithm using off the shelf techniques could significantly reduce the
developmental time and therefore the cost of its implementation.

The convergence speed of Dimension Growth in KDAC is slow. As we know from
optimization theory, the convergence rate for gradient methods heavily depend
on the conditional value of the Hessian matrix. The conditional value is
defined as the ratio between the maximum and the minimum eigenvalue of the
Hessian matrix.
\[ \tmop{condition} \tmop{value} = \frac{\tmop{eig}_{\max} ( \nabla^2 f (
   x))}{\tmop{eig}_{\min} ( \nabla^2 f ( x))} \]


The ideal condition value is when the ratio is equal to 1 and the convergence
rate slows down very quickly as we increase the condition value beyond 10.
Given this fact, it would be instructive to study the Hessian matrix to
potentially explain the slow convergence of gradient methods. The Hessian for
each column $r$ has the following form :
\[ \nabla^2 f ( w) = \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{w_r^T A_{i, j} w_r}{2 \sigma^2}} \left[ A_{i, j} - \frac{1}{\sigma^2}
   A_{i, j} w_r w_r^T A_{i, j} \right] \]
From the Hessian matrix, we see that the condition value depends on the
summation of matrix $A_{i, j}$ and $A_{i, j} w$ multiplied by some constant
term, $\frac{\gamma_{i, j}}{\sigma^2} e^{- \frac{w_r^T A_{i, j} w_r}{2
\sigma^2}}$. From this form, it is intuitive to find clues to size of the
conditional value from the individual behaviors of $A_{i, j}$ and $A_{i, j}
w$.



Due to the complexity of the Hessian form, it may be sufficiently instructive
to simply look for an approximation of the Hessian to study its conditional
value behavior. Given the problem :
\begin{equation}
  f ( w) = - \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T A_{i, j} w}{2 \sigma^2}}
\end{equation}
We could approximate the equation (2), by using the Taylor Expansion around 0.
Since gradient methods are techniques using the 1st order approximation, we
could similarly make a 1st order approximate of the original function.
\begin{equation}
  f ( w) \approx - \sum_{i, j} \gamma_{i, j}  \left( 1 - \frac{w^T A_{i, j}
  w}{2 \sigma^2} \right)
\end{equation}
At this point, we find the approximate Hessian by taking the 2nd derivative.
\[ \nabla f ( w) \approx \sum \frac{\gamma_{i, j}}{\sigma^2} A_{i, j} w \]
\[ \nabla^2 f ( w) \approx \sum \frac{\gamma_{i, j}}{\sigma^2} A_{i, j} \]
From this form, we further simplied the Hessian matrix. And the simplified
Hessian suggests a dominant influence of the summation of the $A_{i, j}$
matrix with some constant value $\gamma_{i, j} / \sigma^2$.



At this point, let's take a step back and ask how the eigenvalues of $A_{i,
j}$ and $A_{i, j} w_r$ influence the conditional value depending on the type
of data we handle. \ We first note that the $A_{i, j}$ matrix is formed with
the following equation.
\[ A_{i, j} = ( x_i - x_j) ( x_i - x_j)^T \]
Given a single gaussian cluster of data :

\begin{figure}[h]
  \resizebox{300px}{300px}{\includegraphics{FKDAC-1.eps}}
  \caption{}
\end{figure}

If we calculate
\[ A = \sum A_{i, j} \]
If we randomly generate 100 similar distributions with randomize mean,
variance, sample size and dimension, the condition value stays within a small
bounded range.

\begin{figure}[h]
  \resizebox{600px}{400px}{\includegraphics{FKDAC-2.eps}}
  \caption{}
\end{figure}

This plot is generated with the following code :

{\noindent}\begin{tmindent}
  \begin{tmparsep}{0em}
    
    
    
    
    cv = []
    
    for k = 1:100
    
    \ \ \ \ \ \ \ n = 20; \%floor(40*rand());
    
    \ \ \ \ \ \ \ d = floor(5*rand());
    
    \ \ \ \ \ \ \ A = zeros(d,d);
    
    \ \ \ \ \ \ \ p = floor(5*rand())*randn(n,d);
    
    \ \ \ \ \ \ \ for m = 1:n
    
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ for n = 1:n
    
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v = p(m,:) - p(n,:);
    
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ A = A + v'*v;
    
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ end
    
    \ \ \ \ \ \ \ end
    
    \ \ \ \ \ \ \
    
    \ \ \ \ \ \ \ [U,S,V] = svd(A);
    
    \ \ \ \ \ \ \ D = diag(S) + 1;
    
    
    
    \ \ \ \ \ \ \ \%max(D)/min(D)
    
    \ \ \ \ \ \ \ cv = [cv max(D)/min(D)];
    
    \ \ \ \ \ \ \
    
    end
    
    
    
    plot(cv)
    
    xlabel('iteration')
    
    ylabel('conditional value')
    
    title('randomize sample size, variance, dimension for condition value with
    100 iterations')
  \end{tmparsep}
\end{tmindent}{\hspace*{\fill}}{\medskip}



From the plot of the conditional value, we can conclude that the conditional
value stays bounded regardless of the sample size, variance, mean and
dimensionality. This is for the case of a single cohesive cluster. However,
what would happen if there are multiple clusters? \ As we vary the number of
clusters as well as their distance apart, a clear pattern imerges. \ The
following plot shows how the conditional value of 2 and 3 gaussian
distributions. As we move them further apart from each other, the condition
value explodes very quickly.

\begin{figure}[h]
  \resizebox{600px}{400px}{\includegraphics{FKDAC-3.eps}}
  \caption{}
\end{figure}

In conjunction to the plot of a single cluster, this plot suggests that the
conditional value is not bounded when the data type are separate into
different clusters. As a matter of fact, the more clusters are involved the
faster the conditional value grows. \ \ \ The purpose of showing
ill-conditionality of this problem provide a reasoning to avoid Gradient
method as an optimization approach. This allows us to lead into using a
different optimization all together.

\section{Fast KDAC}



FKDAC is an alternative approach to Dimension Growth that estimates the
optimal result without using gradient methods. We restate the optimization
problem here :
\begin{equation}
  \begin{array}{ll}
    \min & - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} ( W^T A_{i, j}
    W)}{2 \sigma^2}}\\
    s.t & W^T W = I\\
    & W \in \mathbbm{R}^{d \times q}\\
    & A \in \mathbbm{R}^{d \times d}\\
    & \gamma_{i, j} \in \mathbbm{R}
  \end{array}
\end{equation}




\subsection{Optimality Conditions}



Due to the non-convex nature of the problem, FKDAC cannot guarentee to
discover the global minimum. However, a sufficient optimality condition exists
for a local minimum. Given the problem :
\[ \begin{array}{ll}
     \min & f ( x)\\
     x & \\
     s.t & h ( x) = 0
   \end{array} \]
There exists a sufficiency condition according to Bertsekas in [1],
Proposition 3.2.1. It states that $x^{\ast}$ is a optimal solution of it
satisfies the following proposition.



\begin{proposition}
  (Second Order Sufficiency Conditions )
  
  Assume that f and h are twice continuously differentiable, and let
  $x^{\ast} \in \mathbbm{R}^n$ and $\lambda^{\ast} \in \mathbbm{R}^m$ satisfy
  the following 3 conditions :
  \[ \nabla_x \mathcal{L} ( x^{\ast}, \lambda^{\ast}) = 0
     \begin{array}{llllllllllllllllllllllllllllllll}
       &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &
       &  &  &  &  &  &  & 
     \end{array} \tmop{Condition} 1 \]
  \[ \nabla_{\lambda} \mathcal{L} ( x^{\ast}, \lambda^{\ast}) = 0
     \begin{array}{llllllllllllllllllllllllllllllll}
       &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &
       &  &  &  &  &  &  & 
     \end{array} \tmop{Condition} 2 \]
  \[ z^T \nabla_{x x}^2 L ( x^{\ast}, \lambda^{\ast}) z > 0
     \begin{array}{llllllll}
       &  &  & \tmop{for} & \tmop{all} & z \neq 0 & \tmop{with} & \nabla h (
       x^{\ast})^T z = 0
     \end{array} \begin{array}{lll}
       &  & 
     \end{array} \tmop{Condition} 3 \]
  
  
  \tmtextbf{Then $x^{\ast}$ is a strict local minimum of $f$ subjected to h(x)
  = 0.}
\end{proposition}



Notice that this is a very general sufficiency assumption that does not assume
the form of the functions $f$ or $h$. Using this requirement as a proof of
optimality, we define how our cost function can satisfy the definition. We
start by finding the gradient of the Lagrangian with respect to $w$.
\[ \mathcal{L} = - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} ( W^T A_{i,
   j} W)}{2 \sigma^2}} - \frac{1}{2} \tmop{Tr} [ \Lambda (W^T W - I)] \]
\[ \frac{\partial \mathcal{L}}{\partial W} = \left[ \sum_{i, j}
   \frac{\gamma_{i, j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( W^T A_{i, j} W)}{2
   \sigma^2}} A_{i, j} W \right] - W \Lambda = 0 \]
\[ \frac{\partial \mathcal{L}}{\partial W} = \left[ \sum_{i, j}
   \frac{\gamma_{i, j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( W^T A_{i, j} W)}{2
   \sigma^2}} A_{i, j}  \right] W = W \Lambda \]




If we let :
\[ \Phi ( W) = \left[ \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{\tmop{Tr} ( W^T A_{i, j} W)}{2 \sigma^2}} A_{i, j} \right] \]
We can rewrite the equation :


\begin{equation}
  \Phi ( W^{\ast}) W^{\ast} = W^{\ast} \Lambda
\end{equation}


The equation above implies that the gradient of the Lagrangian is equal to 0
when $W^{\ast}$ have columns of \ eigenvectors of $\Phi ( w^{\ast})$. \
Meeting this condition will satisfy the 1st condition of Proposition 1.



The 2nd condition of Proposition 1 requires that : $\nabla_{\Lambda}
\mathcal{L}= 0$. From the cost function, we get :
\begin{equation}
  \frac{\partial \mathcal{L}}{\partial \Lambda} = 1 - W^T W = 0
\end{equation}


With the conclusions from equation (5), it is obvious that if the optimal
$W^{\ast}$ have columns as \ unit norm eigenvectors, this condition is
automatically satisfied.



To satisfy the final condtion, we first define :
\[ \mathcal{D}f ( X) [ Z] \assign \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{f ( X + t Z) - f ( X)}{t} \]


We want to define a set of $Z$ such that :
\[ \nabla h ( x^{\ast})^T Z = 0 \]


We know that $h ( W) = W^T W - 1$, therefore :
\[ \mathcal{D}h ( W) [ Z] = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{( W + t Z)^T ( W + t Z) - W^T W}{t} = Z^T W + W^T Z \]


From this, we define as set of $Z$ such that :
\[ S_Z = \{ Z : Z^T W + W^T Z = 0 \} \]


The 3rd optimality condition is therefore met when :
\[ \tmop{Tr} ( Z^T \mathcal{D}f ( W) [ Z]) - \tmop{Tr} ( \Lambda Z^T Z) > 0
   \begin{array}{llll}
     &  &  & 
   \end{array} \forall Z \in S_Z \]


In Simpler terms, if the 2nd order gradient of the Lagrangian is Positive
Definite, then the condition is satisfied. Due to the complexity of computing
the Hessian in real time, a relaxation approach is discussed in a later
section.





\subsection{1st Order Relaxation}



Given that $\Phi ( W)$ is a function of $W$, it is difficult to take advantage
of the eigenvalue/eigenvector relationship to find the solution.
\[ \Phi ( W^{\ast}) W^{\ast} = W^{\ast} \Lambda \]


A standard approach to circumvent this restriction is to treat $W^{\ast}$
inside $\Phi$ as a separate variable and solve the problem iteratively.
\[ \Phi ( W_{k - 1}) W = W \Lambda \]


The key is, therefore, finding a reasonable initialization value for $W_0$ .
This section provide a theoretical suggestion for a logical initialization
point. \ Given the following problem :
\[ \begin{array}{l}
     \min\\
     W
   \end{array} - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T A_{i, j}
   W)}{2 \sigma^2}} \]
\[ s.t \begin{array}{lll}
     &  & 
   \end{array} W^T W = I \]
Writing out the Larangian :
\[ \mathcal{L} = - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T A_{i,
   j} W)}{2 \sigma^2}} + \frac{1}{2} \tmop{Tr} (\Lambda (I - W^T W)) \]
We simplify the Lagrangian by using the Taylor approximation on the
problematic exponential term. The Taylor approximation is expanded up to the
1st order centering around 0.
\[ \mathcal{L} \approx - \sum_{i, j} \gamma_{i, j}  \left( 1 - \frac{\tmop{Tr}
   (W^T A_{i, j} W)}{2 \sigma^2} \right) + \frac{1}{2} \tmop{Tr} (\Lambda (I -
   W^T W)) \]
Following the similar procedure by finding the derivative of $\mathcal{L}$ and
setting it to zero.
\[ \nabla \mathcal{L} \approx \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} A_{i,
   j} W - W \Lambda = 0 \]
We arrange the problem into standard eigenvalue/eigenvector problem.
\[ \left[ \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} A_{i, j} \right] W = W
   \Lambda \]
From this, we see that $\Phi$ is no longer a function of $W$. If $W \in
\mathbbm{R}^{d \times q}$, the $W_0$ is therefore, $q$ eigenvectors of the
matrix $\Phi_0 $:
\[ \Phi_0 = \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} A_{i, j} \]
And $\Lambda$ is the eigenvalue matrix.
\[ \Lambda = \tmop{eig} \left[ \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2}
   A_{i, j} \right] \]
Using the $W_0$ discovered from this initialization point, we could similarly
form an updated $\Phi_{n + 1}$ using the original cost derivative without any
approximation.
\[ \Phi_{n + 1} = \left[ \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{\tmop{Tr} (W_n^T A_{i, j} W_n)}{2 \sigma^2}} A_{i, j} \right] \]
Given that only $q$ eigenvectors are chosen out of $d$ total eigenvectors, and
$q \ll d$. The total possible combinations could be extremely large depending
on the values of $d$ and $q$, more specifically it is equal to $\left(
\begin{array}{l}
  d\\
  q
\end{array} \right)$. Solving this problem becomes a difficult and separate
combinatorial optimization. \ One reasonable approach is to use the greedy
algorithm to find $q$ eigenvectors that produces the lowest cost. \ However,
as we explore the theoretic motivation for choosing the optimal eigenvectors,
we demonstrate the possibility of knowing the optimal eigenvectors without
resorting to greedy methods.



\subsection{Convergence}



The convergence of the sequence generated from using the 1st order relaxation
may not be possible, however through the Bolzano-Weierstrass Theorem, a
convergent subsequence can be shown. According to [1], \ it states that :

\begin{theorem}
  A bounded sequence of real numbers has convergent subsequence.
\end{theorem}

According to this theorem, if we can show that the sequences generated from
the 1st order relaxation is bounded, it has a convergent subsequence. If we
study the equation more closely :
\[ \Phi ( W_{k - 1}) W_k = W_k \Lambda \]
S. D. Bay, {\textquotedblleft}The UCI KDD archive,'' 1999. [Online].
Available:



http://kdd.ics.uci.edu



The key driver of the sequence of $W_k$ is the matrix $\Phi$, therefore, if we
can show that if this matrix is bounded, the sequence itself is also bounded.
We look inside the construction of the matrix itself.
\[ \Phi_{n + 1} = \left[ \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{\tmop{Tr} (W_n^T A_{i, j} W_n)}{2 \sigma^2}} A_{i, j} \right] \]


From this, equation, start with the matrix $A_{i, j} = ( x_i - x_j) ( x_i -
x_j)^T$. Since $x_i, x_j$ are data points that are always centered and scaled
to a variance of 1, the size of this matrix is always constrained. It also
implies that $A_{i, j}$ is a PSD matrix. From this, the exponential term $e^{-
\frac{\tmop{Tr} (W_n^T A_{i, j} W_n)}{2 \sigma^2}} $is always limited between
the value of 0 and 1. The value of $\sigma$ is a constant given from the
initialization stage. Lastly, we have the $\gamma_{i, j}$ term. Looking at it
closely, it is equal to :
\[ \gamma_{i, j} = \frac{u_i^T u_j}{d_i d_j} - \lambda ( H Y Y^T H)_{i, j} \]


The vectors $u_i$ will always be a vectors with a norm of 1, $d_i$ is always
less than $\sqrt{N}$, where $N$ is the number of samples. $\lambda$ is a
constant given from the initialization stage. $H$ and $Y$ can be considered as
constants as well. \ From this, it is clear that the infinity norm of the
$\Phi$ is always bounded. The eigenvalue matrix of $\Lambda$ is therefore also
bounded. Using the Bolzano-Weierstrass Theorem, the sequence contains a
convergent subsequence.



\subsection{2nd Order Condition}



As mentioned in the previous section, knowing that picking the eigenvectors of
$\Phi ( W^{\ast})$ satisfies the 1st order condition is not sufficient.
Ideally, we want to pick the optimal eigenvectors. The purpose of this section
is to provide the 2nd order condition and prove the following theorem.



\begin{theorem}
  Given a large enough $\sigma$, picking the smallest q eigenvectors of $\Phi
  ( W^{\ast})$ satisfies the 2nd order condition.
\end{theorem}



\begin{proof}
  
\end{proof}

Given :
\[ \mathcal{L}= - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T A_{i,
   j} W)}{2 \sigma^2}} - \frac{1}{2} \tmop{Tr} ( \Lambda ( W^T W - I)) \]
\[ \nabla \mathcal{L}= \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{\tmop{Tr} (W^T A_{i, j} W)}{2 \sigma^2}} A_{i, j} W - W \Lambda \]

\[ \mathcal{D} \nabla \mathcal{L} [ Z] = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{\partial}{\partial t} \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W + t Z)^T A_{i, j}  ( W + t Z))}{2
   \sigma^2}} A_{i, j}  ( W + t Z) - ( W + t Z) \Lambda \]


Let's divide this problem into 3 terms.
\[ \mathcal{D} \nabla \mathcal{L} [ Z] = T_1 + T_2 - T_3 \]


Let's now solve each term separately.
\[ T_1 = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{\partial}{\partial t} \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W + t Z)^T A_{i, j}  ( W + t Z))}{2
   \sigma^2}} A_{i, j} W \]
\[ T_1 = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{\partial}{\partial t} \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W^T A_{i, j} W + t Z^T A_{i, j} W +
   t W^T A_{i, j} Z + t^2 Z^T A_{i, j} Z \nobracket)}{2 \sigma^2}} A_{i, j} W
\]
\[ T_1 = - \sum_{i, j} \frac{\gamma_{i, j}}{2 \sigma^4} e^{- \frac{\tmop{Tr} (
   ( W^T A_{i, j} W \nobracket)}{2 \sigma^2}} \tmop{Tr} ( Z^T A_{i, j} W + W^T
   A_{i, j} Z) A_{i, j} W \]

\[ T_2 = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{\partial}{\partial t} \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} t e^{- \frac{\tmop{Tr} ( ( W + t Z)^T A_{i, j}  ( W + t
   Z))}{2 \sigma^2}} A_{i, j} Z \]
\[ T_2 = \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{- \frac{\tmop{Tr} (W^T
   A_{i, j} W)}{2 \sigma^2}} A_{i, j} Z \]
\[ T_3 = \begin{array}{l}
     \lim\\
     t \rightarrow 0
   \end{array} \frac{\partial}{\partial t}  ( W + t Z) \Lambda \]
\[ T_3 = Z \Lambda \]


Putting together all 3 terms :
\[ \mathcal{D} \nabla \mathcal{L} [ Z] = \left\{ \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2
   \sigma^2}}  \left[ A_{i, j} Z - \frac{1}{2 \sigma^2} \tmop{Tr} ( Z^T A_{i,
   j} W + W^T A_{i, j} Z) A_{i, j} W \right] \right\} - Z \Lambda \]


Since $A_{i, j}$ is symmetric, we know that $\tmop{Tr} ( Z^T A_{i, j} W) =
\tmop{Tr} ( W^T A_{i, j} Z)$, we can combine the two terms.
\[ \mathcal{D} \nabla \mathcal{L} [ Z] = \left\{ \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2
   \sigma^2}}  \left[ A_{i, j} Z - \frac{1}{\sigma^2} \tmop{Tr} ( Z^T A_{i, j}
   W) A_{i, j} W \right] \right\} - Z \Lambda \]




We know that the 2nd order condition is satisfied if :
\begin{equation}
  \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) > 0
\end{equation}


For all $Z$ satisfying the following condition.
\begin{equation}
  Z^T W + W^T Z = 0
\end{equation}


Therefore :
\[ \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) = \left\{ \sum_{i, j}
   \frac{\gamma_{i, j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W^T A_{i, j} W
   \nobracket)}{2 \sigma^2}}  \left[ \tmop{Tr} ( Z^T A_{i, j} Z) -
   \frac{1}{\sigma^2} \tmop{Tr} ( Z^T A_{i, j} W)^2 \right] \right\} -
   \tmop{Tr} ( Z^T Z \Lambda_W) \]


Or it is easier to read them in 3 parts.
\[ \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) = \tmop{Part} 1 +
   \tmop{Part} 2 + \tmop{Part} 3 \]
\[ \tmop{Part} 1 = \tmop{Tr} \left( Z^T \sum_{i, j} \frac{\gamma_{i,
   j}}{\sigma^2} e^{- \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2
   \sigma^2}} A_{i, j} Z \right) \]

\[ \tmop{Part} 2 = - \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^4} e^{-
   \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2 \sigma^2}} \tmop{Tr} (
   Z^T A_{i, j} W)^2 \]
\[ \tmop{Part} 3 = - \tmop{Tr} ( Z^T Z \Lambda_W) \]




Because $Z$ has the same dimension as $W$, each column of $Z$ resides in the
space of $\mathbbm{R}^d$. Since the eigenvectors of $\Phi ( W)$ spans the
$\mathbbm{R}^d$, each column of $Z$ can be represented as a linear combination
of the eigenvalues. For the sake of clearity, we reiterate that $W$ as the $q$
eigenvectors chosen during each iteration, and $\bar{W}$ as the $d - q$
eigenvectors not chosen. Each column of $Z$ can therefore be written as :
\[ Z = \left[ \begin{array}{llll}
     z_1 & z_2 & \ldots & z_q
   \end{array} \right] \]
\[ z_i = \sum_{\alpha}^{d - q} \rho^{( i)}_{\bar{W}_{\alpha}} 
   \bar{W}_{\alpha} + \sum_{\beta}^q \rho^{( i)}_{W_{\beta}} W_{\beta} \]
\[ z_i = \bar{W} P_{\bar{W}}^{( i)} + W P_W^{( i)} \]


$\bar{W}_{\alpha}$ denotes the $\alpha$th column of $\bar{W}$ matrix, and
$W_{\beta}$ denotes the $\beta \tmop{th}$ column of the $W$ matrix. $\rho^{(
i)}_{\bar{W}_{\alpha}}$ denotes the coefficient for the vector
$\bar{W}_{\alpha}$ for the $z_i$ vector. $\rho^{( i)}_{\bar{W}_{\beta}}$
denotes the coefficient for the vector $\bar{W}_{\beta}$ for the $z_i$ vector.
\ To further simplify the notation, we elimated the summation and turned
$\rho^{( i)}_{\bar{W}_{\alpha}}$ into a vector $P_{\bar{W}}^{( i)}$.





Let plug this definition of $Z$ into each part.



Part 1
\[ \tmop{Tr} \left( Z^T \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^2} e^{-
   \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2 \sigma^2}} A_{i, j} Z
   \right) \]
\[ \tmop{Tr} ( Z^T \Phi ( W) Z) \]
\[ = \tmop{Tr} \left( Z^T \Phi ( W) \left[ \begin{array}{lllll}
     z_1 & z_2 & \ldots & \ldots & z_q
   \end{array} \right] \right) \]
\[ = \tmop{Tr} \left( Z^T \Phi ( W) \left[ \begin{array}{lllll}
     ( \bar{W} P_{\bar{W}}^{( 1)} + W P_W^{( 1)}) & ( \bar{W} P_{\bar{W}}^{(
     2)} + W P_W^{( 2)}) & \ldots & \ldots & ( \bar{W} P_{\bar{W}}^{( q)} + W
     P_W^{( q)})
   \end{array} \right] \right) \]
\[ = \tmop{Tr} \left( Z^T \left[ \begin{array}{llll}
     ( \Phi ( W) \bar{W} P_{\bar{W}}^{( 1)} + \Phi ( W) W P_W^{( 1)}) & \ldots
     & \ldots & ( \Phi ( W) \bar{W} P_{\bar{W}}^{( q)} + \Phi ( W) W P_W^{(
     q)})
   \end{array} \right] \right) \]
\[ = \tmop{Tr} \left( Z^T \left[ \begin{array}{llll}
     ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( 1)} + W \Lambda P_W^{( 1)}) &
     \ldots & \ldots & ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( q)} + W \Lambda
     P_W^{( q)})
   \end{array} \right] \right) \]
\[ = \tmop{Tr} \left( \left[ \begin{array}{lll}
     ( \bar{W} P_{\bar{W}}^{( 1)} + W P_W^{( 1)}) & \ldots & ( \bar{W}
     P_{\bar{W}}^{( q)} + W P_W^{( q)})
   \end{array} \right]^T \right. \ldots \]
\[ \left[ \begin{array}{lll}
     ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( 1)} + W \Lambda P_W^{( 1)}) &
     \ldots & ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( q)} + W \Lambda P_W^{(
     q)})
   \end{array} \right] \]
\[  \]
\[ = \tmop{Tr} \left( \left[ \begin{array}{l}
     ( P_{\bar{W}}^{( 1)^T} \bar{W}^T + P_W^{( 1)^T} W^T)\\
     \ldots\\
     ( P_{\bar{W}}^{( q)^T} \bar{W} ^T + P_W^{( q)^T} W^T)
   \end{array} \right] \right. \ldots \]
\[ \left[ \begin{array}{lll}
     ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( 1)} + W \Lambda P_W^{( 1)}) &
     \ldots & ( \bar{W} \bar{\Lambda} P_{\bar{W}}^{( q)} + W \Lambda P_W^{(
     q)})
   \end{array} \right] \]
\[ = ( P_{\bar{W}}^{( 1)^T} \bar{\Lambda} P_{\bar{W}}^{( 1)} + P_{\bar{W}}^{(
   2)^T} \bar{\Lambda} P_{\bar{W}}^{( 2)} + \ldots + P_{\bar{W}}^{( q)^T}
   \bar{\Lambda} P_{\bar{W}}^{( q)} ) + ( P_W^{( 1)^T} \Lambda P_W^{( 1)} +
   P_W^{( 2)^T} \Lambda P_W^{( 2)} + \ldots + P_W^{( q)^T} \Lambda P_W^{( q)}
   ) \]
\[ = \tmop{Tr} \left( \left[ \begin{array}{l}
     P_{\bar{W}}^{( 1)^T}\\
     P_{\bar{W}}^{( 2)^T}\\
     \ldots\\
     P_{\bar{W}}^{( q)^T}
   \end{array} \right] \bar{\Lambda} \left[ \begin{array}{llll}
     P_{\bar{W}}^{( 1)} & P_{\bar{W}}^{( 2)} & \ldots & P_{\bar{W}}^{( q)}
   \end{array} \right]  \right) + \tmop{Tr} \left( \left[ \begin{array}{l}
     P_W^{( 1)^T}\\
     P_W^{( 2)^T}\\
     \ldots\\
     P_W^{( q)^T}
   \end{array} \right] \Lambda \left[ \begin{array}{llll}
     P_W^{( 1)} & P_W^{( 2)} & \ldots & P_W^{( q)}
   \end{array} \right]  \right) \]
\[ = \tmop{Tr} ( P_{\bar{W}}^T  \bar{\Lambda} P_{\bar{W}} ) + \tmop{Tr} (
   P_W^T \Lambda P_W ) \]


There's not much to simplify for Part 2, we leave it alone.
\[ \tmop{Part} 2 = - \sum_{i, j} \frac{\gamma_{i, j}}{\sigma^4} e^{-
   \frac{\tmop{Tr} ( ( W^T A_{i, j} W \nobracket)}{2 \sigma^2}} \tmop{Tr} (
   Z^T A_{i, j} W)^2 \]


Par 3 also have a $Z$ term that could be simplified, using the same logic as
Part 1, we can also simplify it.
\[ \tmop{Part} 3 = - \tmop{Tr} ( Z^T Z \Lambda_W) \]
\[ \tmop{Part} 3 = - \tmop{Tr} ( Z \Lambda_W Z^T) \]
\[ = \tmop{Tr} ( P_{\bar{W}}^T \Lambda P_{\bar{W}} ) + \tmop{Tr} ( P_W^T
   \Lambda P_W ) \]


Together :
\[ \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) = \tmop{Tr} (
   P_{\bar{W}}^T  \bar{\Lambda} P_{\bar{W}} ) + \tmop{Tr} ( P_W^T \Lambda P_W
   ) - \tmop{part} 2 - \tmop{Tr} ( P_{\bar{W}}^T \Lambda P_{\bar{W}} ) -
   \tmop{Tr} ( P_W^T \Lambda P_W ) \]


We can cancel out the term Tr $( P_W^T \Lambda P_W )$ and combine the term
$\tmop{Tr} ( P_{\bar{W}}^T \Lambda P_{\bar{W}} )$.


\[ \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) = \tmop{Tr} (
   P_{\bar{W}}^T  ( \bar{\Lambda} - \Lambda) P_{\bar{W}} ) - \tmop{part} 2 \]


Notice that we can shift the trace term :
\[ \tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z]) = \tmop{Tr} ( (
   \bar{\Lambda} - \Lambda)  ( P_{\bar{W}} P_{\bar{W}}^T) ) - \tmop{part} 2 \]


Putting it into this form allows us to look at the trace as an element wise
product of the 2 matrices, $( \bar{\Lambda} - \Lambda) $ \ and $P_{\bar{W}}
P_{\bar{W}}^T $. Since $( \bar{\Lambda} - \Lambda) $ is a diagonal matrix with
all other elements equal to 0, only the diagonal terms of $P_{\bar{W}}
P_{\bar{W}}^T $ will factor into element-wised sum. \ Since the diagonal of
$P_{\bar{W}} P_{\bar{W}}^T $ will always be positive, if $( \bar{\Lambda} -
\Lambda) $ is a positive diagonal matrix, the resulting trace will always be
positive. The 2nd order condition is, therefore, satsified, when :


\begin{equation}
  \tmop{Tr} ( ( \bar{\Lambda} - \Lambda)  ( P_{\bar{W}} P_{\bar{W}}^T) ) >
  \tmop{part} 2
\end{equation}




From the formula above, to satisfy the 2nd order condition, our goal is to
make $\tmop{Tr} ( Z^T \mathcal{D} \nabla \mathcal{L} [ Z])$ as large as
possible. It would make sense, therefore, to pick $\Lambda_W$ to be as small
as possible to maximize the left hand side of the equation. \ From this
relationship, we conclude that if the ratio of the eigenvalues not chosen is
sufficiently greater than the chosen eigenvalues, the 2nd order condition is
satisfied. Further more, notice that part 2 has a $\frac{1}{\sigma^4}$ term.
For a largen enough sigma, part 2 approach 0. Therefore, since the left hand
side is always positive, picking the smallest eigenvectors automatically
satisfies the 2nd order condition. This proof's theorem 3.



The ineqality implies that if the gap between the eigenvalues chosen and the
eigenvalues not chosen is larger than part 2, the 2nd order condition is
satisfied. To characterize this gap, it is reasonable to measure the
difference between the largest eigenvalue from $\Lambda_W$ against the
smallest eigenvalue from $\bar{\Lambda}$. We will call this difference the
threshold eigengap. To further study the inequality, experiments were
conducted to identify the threshold eigengap percent increase in each
experiment.



The following table lists the threshold eigengap percent increase for multiple
experiments. The percent increase is calculated by dividing the threshold gap
by the largest eigenvalue of $\Lambda_W$.


\[ \tmop{Eigen} \tmop{Gap} \% \tmop{increase} \]
\[ \begin{array}{|l|l|}
     \hline
     \tmop{Experiment} & \tmop{Percent} \tmop{increase}\\
     \hline
     \tmop{Small} 4 \tmop{Gaussians} & 559, 604\%\\
     \hline
     \tmop{Large} 4 \tmop{Gaussians} & 1, 479\%\\
     \hline
     \tmop{Moon} \tmop{no} \tmop{noise} & 6, 761\%\\
     \hline
     \tmop{Moon} \tmop{with} \tmop{noise} & 660\%\\
     \hline
     \tmop{Breast} \tmop{Cancer} \tmop{Data} & 211\%\\
     \hline
     \tmop{Facial} \tmop{image} \tmop{Data} & 113.8\%\\
     \hline
   \end{array} \]






It is from this derivation, that each iterative process, the smallest
eigenvectors are chosen as the new $W$ matrix.



It is interesting to note that this formuation points to potential logical
values for $q$. Locations where the eigengap percent increase is large have
the highest chance of satisfying the 2nd order sufficiency condition.



\section{Results}



The following plot is a speed test generated by running various experiement
using the different bench mark algorithms. Since all the bench mark algorithms
are gradient based, they are prone to be stuck in a local minimum. To achieve
better result, random initialization is required to search for a global
optimal. For these experiemnts, 30 random initializations were required. The
benefit of FKDAC is that since a theoretical initialization is provided, it
achieves the same optimal result without multiple random initializations.

\begin{figure}[h]
  \resizebox{800px}{500px}{\includegraphics{FKDAC-4.eps}}
  \caption{}
\end{figure}

\begin{figure}[h]
  \resizebox{800px}{500px}{\includegraphics{FKDAC-5.eps}}
  \caption{}
\end{figure}

\section{Approximating $w_{k + 1}$}

Let $q$ be the reduced the dimension and $d$ as the original dimension. We
have seen from the previous section that the stationary poins could be found
by picking $q$ eigenvectors from the $\Phi$ marix. However, since $q$ could be
significantly smaller than $d$, using the appropriate eigenvectors could
effect the outcome. \ Without any prior knowledge, the problems requires the
optimal solution to choose from potenial large outcomes of $\left(
\begin{array}{l}
  d\\
  q
\end{array} \right)$. To simpliy the selection process, this section provides
a theoretical argument on how these vectors could be chosen intelligently. To
demonstrate the idea, we start by simplifying the problem into a single column
$w$ vector. This is without much loss of generality since the idea could be
generalized into higher dimensions. Again, assuming that the Lipschitz
condition holds, we start by taking the 1st order Taylor Expansion around some
$w_k$.
\begin{equation}
  f (w) = - \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T_k A_{i, j} w_k}{2
  \sigma^2}} + 2 \left[ \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T_k A_{i, j}
  w_k}{2 \sigma^2}} A_{i, j} w_k \right]^T  (w_{k + 1} - w_k)
\end{equation}
The approximation generally assume that the constraint space is convex. For
our case, since our constraint space is not convex, we must assume at least
that the constraint space is convex within a certain radius.
\[ w \in S \begin{array}{llllllllll}
     &  & s.t &  & S & \tmop{is} & \tmop{convex} &  & \forall & 
   \end{array} \|w - w_k \| \leq \varepsilon \]
Looking at the right side of equation 6, the first term is identical to the
current cost at $w_k$. \ If we assume that higher order terms are
insignificant, we can achieve a lower cost as long as we pick $w_{k + 1}$ such
that the 2nd term is a negative value. Let's look at the 2nd term more
closely.
\[ 2 \left[ \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T_k A_{i, j} w_k}{2
   \sigma^2}} A_{i, j} w_k \right]^T  (w_{k + 1} - w_k) \leq 0 \]
Note that $A_{i, j}$ are symmetric. We can rewrite this expression :
\begin{equation}
  w_k^T \left[ \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T_k A_{i, j} w_k}{2
  \sigma^2}} A_{i, j} \right] w_{k + 1} \leq w_k^T \left[ \sum_{i, j}
  \gamma_{i, j} e^{- \frac{w^T_k A_{i, j} w_k}{2 \sigma^2}} A_{i, j} \right]
  w_k
\end{equation}
Looking at the eqution above, variously conclusions could be drawn. First, by
realizing that the quation :
\[ v^T = w_k^T \left[ \sum_{i, j} \gamma_{i, j} e^{- \frac{w^T_k A_{i, j}
   w_k}{2 \sigma^2}} A_{i, j} \right] \]
is a vector, we could rewrite equation (7) as :
\[ v^T w_{k + 1} \leq v^T w_k \]
To draw the first conclusion, we note that $v^T w_{k + 1}$ is a product of two
vectors. To ensure that $v^T w_{k + 1}$ is always less than or equal to $v^T
w_k$, we simply pick $w_{k + 1}$ that minimizes the term $v^T w_{k + 1}$.
\[ \begin{array}{l}
     \min\\
     w_{k + 1}
   \end{array} v^T w_{k + 1} \]
We know from geometry that the product of two vectors is minimized when they
are opposite directions of each other. Therefore $w_{k + 1}$ is minimized when
$w_{k + 1} = - v$. From this approach, we can iteratively pick $w_{k + 1}$ by
setting $w_{k + 1} = - v$ to achieve a descending direction. However, this
approach encounters the problem of discovering the step length accompanying
the descending direction. Since the constraint space is not convex, the step
length cannot be discovered by methods of interpolation.



Although finding a definite descent direction that stays on the Stiefel
manifold is difficult, another insight could be drawn by making some obvious
observations of the following equation.
\begin{equation}
  w_k^T \Phi w_{k + 1} \leq w_k^T \Phi w_k
\end{equation}


Observations :



1. We have performed Taylor expansion around $w_k$

2. Any $w_{}$ chosen must have a norm of 1

3. If we have chosen a proper $w_{k + 1}$ that satisfies the inequality, it
forms a descent direction.

4. If we continue to follow the descent direction, we would hit $w^{\ast}$ at
convergence

5. At the point of convergence $w_{k + 1} = w_k$

6. At the point of convergence :
\begin{equation}
  w_{k + 1}^T \Phi w_{k + 1} = w_k^T \Phi w_k
\end{equation}
\ \ \ \ \ \ \ and no vector $v$ exists such that :
\begin{equation}
  w^{\ast T} \Phi w^{\ast} < v^T \Phi v
\end{equation}


Conclusion :



Given the point of expansion $w_k$, there is no need to iteratively approach
convergence because we already know $w^{\ast}$. \ The only vector $w^{\ast}$
that satisfies equation (10) and has a norm of 1, is the least dominant
eigenvector of $\Phi$. Therefore, the least dominant eigenvector of $\Phi$ is
the best approximation of $w^{\ast}$ at the Taylor expansion of $w_k$.



Linking this equation to FKDAC, we have previously shown that the eigenvectors
of $\Phi$ provide a stationary point such that $\nabla \mathcal{L}= 0$. \
However, it was not certain which eigenvectors are most appropriate. From the
conclusion we have just drawn, it shows that the least dominant eigenvectors
are the most reasonable.



\section{Expand the same idea to multiple columns}



After understanding the basic concept through the usage of a single column
example, we can now expand the same idea to a more general multiple column
case. Given the first order Taylor expansion.
\begin{equation}
  f (W) = f (W_k) + \nabla f (W_k)^T  (W - W_k)
\end{equation}
Applying this equation to our problem, we get :
\[ f (W) = f (w) = - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T_k
   A_{i, j} W_k)}{2 \sigma^2}} + \left[ \sum_{i, j} \gamma_{i, j} e^{-
   \frac{\tmop{Tr} (W^T_k A_{i, j} W_k)}{2 \sigma^2}} \tmop{Vec} (A_{i, j}
   W_k) \right]^T \tmop{Vec} (W - W_k) \]
Note that the notation $\tmop{Vec} (.)$ is the vectorization of a matrix.
Similar to the single column case, we only need to concentrate on making the
2nd term negative.
\[ \left[ \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T_k A_{i, j}
   W_k)}{2 \sigma^2}} \tmop{Vec} (A_{i, j} W_k) \right]^T \tmop{Vec} (W - W_k)
   \leq 0 \]
Let :
\[ \beta_{i, j} = \gamma_{i, j} e^{- \frac{\tmop{Tr} (W^T_k A_{i, j} W_k)}{2
   \sigma^2}} \]
\[ \left[ \sum_{i, j} \beta_{i, j} (I \otimes A_{i, j}) \tmop{Vec} (W_k)
   \right]^T  [\tmop{Vec} (W) - \tmop{Vec} (W_k)] \leq 0 \]
We now rearrange the terms and get :
\[ \left[ \sum_{i, j} \beta_{i, j} (I \otimes A_{i, j}) \tmop{Vec} (W_k)
   \right]^T \tmop{Vec} (W) \leq \left[ \sum_{i, j} \beta_{i, j} (I \otimes
   A_{i, j}) \tmop{Vec} (W_k) \right]^T \tmop{Vec} (W_k) \]
\[ \tmop{Vec} (W_k)^T \left[ \sum_{i, j} \beta_{i, j} (I \otimes A_{i, j})
   \right] \tmop{Vec} (W) \leq \tmop{Vec} (W_k)^T \left[ \sum_{i, j} \beta_{i,
   j} (I \otimes A_{i, j}) \right] \tmop{Vec} (W_k) \]
Using the same logic to lower the upper bound, the $W_k$ that minimizes the
upper bound consists of the $q$ least dominant eigenvectors of the matrix
$\Phi$.

\section{Optimality condition}

The Taylor expansion approach corresponds directly with the optimality
condition with a convex constrained space. Given a problem of :
\[ \begin{array}{ll}
     \min & f (x)\\
     x \in X & 
   \end{array} \]
We know that given $x^{\ast}$ as a local minimum in a convex constrained space
of $X$, the following condition must be satisfied.
\[ \nabla f (x^{\ast})^T  (x - x^{\ast}) \geq 0 \begin{array}{lllll}
     &  &  &  & 
   \end{array} \forall x \in X \]
Given that we have a non-convex constraint space, we must make more strict
assumptions. Suppose $x^{\ast}$ is a local minimum within a ball defined as
$\mathcal{B} (x, \varepsilon) \assign \{x : \|x - x^{\ast} \|< \varepsilon\}$.
Assume $\mathcal{B} (x, \varepsilon) \cap X$ is convex and $f$ is convex
within $\mathcal{B} (x, \varepsilon) \cap X$, then we state the following as
the optimality condition.
\[ \nabla f (x^{\ast})^T  (x - x^{\ast}) \geq 0 \begin{array}{lllll}
     &  &  &  & 
   \end{array} \forall x \in X \begin{array}{lll}
     & \tmop{and} & 
   \end{array} \|x - x^{\ast} \| \leq \varepsilon \]
Given a function of :
\[ f (w) = - \sum \gamma_{i, j} e^{- \frac{w^T A_{i, j} w}{2 \sigma^2}} \]
We create an approximate of the function :
\[ f (w) \approx - \sum \gamma_{i, j}  \left( 1 - \frac{1}{2 \sigma^2} w^T
   A_{i, j} w \right) \]
From the approximation, we find the gradient :
\[ \nabla f (w) \approx \sum \frac{\gamma_{i, j}}{\sigma^2} A_{i, j} w \]
Using the approximated gradient, we could now check for the optimality
condition.
\[ w^{\ast T} \left[ \sum \gamma_{i, j} A_{i, j} \right]^T  (w - w^{\ast})
   \geq 0 \]
\[ w^{\ast^T} \left[ \sum \gamma_{i, j} A_{i, j} \right]^T w \geq w^{\ast}
   \left[ \sum \gamma_{i, j} A_{i, j} \right]^T w^{\ast} \]
From the equation above, we see that in order for $w^{\ast}$ to be a local
minimum, $w^{\ast}$ must be chosen such that the left hand side is always
larger than the right hand side for any $w$. This is only possible when
$w^{\ast}$ is the least dominant eigenvector of $\sum \gamma_{i, j} A_{i, j}$
matrix. From this perspective, we conclude that picking the least dominant
eigenvector is a reasonable approximation for the local minimum of the
original cost function.



\section{Implementation Details of Cost function}



The computation of cost function presented in this paper, is a complicated
equation that slows down both the implementation and speed of the results.
\begin{equation}
  \begin{array}{ll}
    \min & - \sum_{i, j} \gamma_{i, j} e^{- \frac{\tmop{tr} ( W^T A_{i, j}
    W)}{2 \sigma^2}}\\
    W & \\
    s.t & W^T W = I\\
    & W \in \mathbbm{R}^{d \times q}\\
    & A \in \mathbbm{R}^{d \times d}\\
    & \gamma_{i, j} \in \mathbbm{R}
  \end{array}
\end{equation}


Instead of solving the function itself, it could be mostly easily done with
the following equation.


\[ \tmop{cost} = \tmop{HSIC} ( X W, U) - \lambda \tmop{HSIC} ( X W, Y) \]


The simplest way is to write a HSIC function, and pass $X W \nocomma, U,$ and
$Y$ to compute the final cost. Although easy, this approach is not the fastest
in terms of separating out the portion of the code that requires constant
update, and the portion that remains constant. In this section, a faster
approach to implement the cost function is outlined.



Starting with the original cost function :
\[ \tmop{cost} = \tmop{HSIC} ( X W, U) - \lambda \tmop{HSIC} ( X W, Y) \]


Convert it into trace format.
\[ \tmop{cost} = \tmop{Tr} ( \tilde{K} H U U^T H) - \lambda \tmop{Tr} (
   \tilde{K} H Y Y^T H) \]


Where $\tilde{K}$ is the normalized kernel of $X W$, which could also be
written as $\tilde{K} = D^{- \frac{1}{2}} K_{X W} D^{- \frac{1}{2}}$. Putting
this into the cost function.
\[ \tmop{cost} = \tmop{Tr} \left( D^{- \frac{1}{2}} K_{X W} D^{- \frac{1}{2}}
   H U U^T H \right) - \lambda \tmop{Tr} \left( D^{- \frac{1}{2}} K_{X W} D^{-
   \frac{1}{2}} H Y Y^T H \right) \]


When optimizing $U$, it is obvious that the 2nd portion does not effect the
optimization. Therefore, $U$ can be solved using the following form.
\[ U = \begin{array}{l}
     \tmop{argmin}\\
     U
   \end{array} \tmop{Tr} ( U^T H D^{- 1 / 2} K_{X W} D^{- 1 / 2} H U) \]


The situation get a bit more complicated if we are optimization for $W$. Using
the combination of the rotation property and the combination of the 2 traces,
the cost can be written as :
\[ \tmop{cost} = \tmop{Tr} ( [ D^{- 1 / 2} H ( U U^T - \lambda Y Y^T) H D^{-
   1 / 2}] K) \]


In this form, it can be seen that the update of $W$ matrix will only affect
the kernel $K$ and the degree matrix $D$. Therefore, it makes sens to treat
the middle portion as a constant which we refer as $\Psi$.
\[ \tmop{cost} = \tmop{Tr} ( [ D^{- 1 / 2} \Psi D^{- 1 / 2}] K) \]


Given that $[ D^{- 1 / 2} \Psi D^{- 1 / 2}]$ is a symmetric matrix, from this
form, we can convert the trace into an element wise product $\odot$.
\[ \tmop{cost} = [ D^{- 1 / 2} \Psi D^{- 1 / 2}] \odot K \]


To further reduction the amount of operation, we let $d$ be a vector of the
diagonal elements of $D^{- 1 / 2}$, hence $d = \tmop{diag} ( D^{- 1 / 2})$,
this equality hold.
\[ D^{- 1 / 2} \Psi D^{- 1 / 2} = [ d d^T] \odot \Psi \]


Therefore, the final cost function can be written in its simplest form as :
\[ \tmop{cost} = \Gamma = \Psi \odot [ d d^T] \odot K \]


During update, as $W$ update during each iteration, the matrix $\Psi$ stays as
a constant while $d d^T$ and $K$ update. The benefit of this form minimize the
complexity of the equation, while simplify cost into easily parallelizable
matrix multiplications. The equation also clearly separates the elements into
portions that require an update and portions that does not.



\section{Implementation Details of the Derivative}



As it was shown from previous sections, the gradient of our cost function
using the Gaussian Kernel has the following form.
\[ \nabla f ( W) = \left[ \frac{1}{\sigma^2} \sum \gamma_{i, j} K_{i, j} A_{i,
   j} \right] W \]


It is often shown as :
\[ \nabla f ( W) = \Phi W \]


The key is therefore to find $\Phi$.
\[ \Phi = \frac{1}{\sigma^2} \sum \gamma_{i, j} K_{i, j} A_{i, j}  \]


If we note that $A_{i, j} = ( x_i - x_j) ( x_i - x_j)^T$ . \ It can be seen
that the inner portion is identical to the cost function. The difference is
the addition of the $A_{i, j}$ matrix and a constant of $\frac{1}{\sigma^2}$.
These extra factors can be incorporate in the following form.
\[ \Phi = \frac{1}{\sigma^2} Q^T \tmop{diag} ( \tmop{Vec} ( \Gamma)) Q \]


Where :
\[ Q = ( X \otimes \tmmathbf{1}_n) - ( \tmmathbf{1}_n \otimes X) \]


Note that $\otimes$ is a tensor product and $\tmmathbf{1}_n$ is a 1 vector
with a length of $n$.



And :
\[ \tmop{cost} = \Gamma = \Psi \odot [ d d^T] \odot K \]


Since $Q$ is a constant that never changes during the optimization, it could
be calculated at the beginning and cached. During each $W$ update using the
gradient, $\Psi$ and $Q$ are considered as constants while $K$ and $D$ require
a constant update. However, each time the $U$ matrix is updated, $\Psi$ must
also be updated.



Here we outline the Algorithm for the $W$ optimization update scheme.



1. Initialize $W_0 = 0$ for the first time, and $W_k$ if $U$ has been
updated.

2. Calculate $Q \nocomma, \Psi$ and store them as constants

3. Calculate $K, D, \Phi$

4. $W_{k + 1} = \overrightarrow{\tmop{eig}}_{\min} ( \Phi)$, pick $q$ least
dominant eigenvectors as $W_{k + 1}$.

5. Repeat 3,4 until $W$ convergence










\begin{itemize}
  [1] \tmtextit{Bartle, Robert G.; Sherbert, Donald R.
  (2000).\tmtextit{Introduction to Real Analysis}(3rd ed.). New York: J.
  Wiley.}
\end{itemize}








\subsection{1.1 Motivation and Previous Work}

Our work is initially motivated by the desire to create an interactive
graphical engine to enchance pattern discovery. To discover multiple hidden
patterns simultaneously, KDAC [8] demonstrated the best performance.
Unfortunately, the optimization approach proposed by the paper proved to be
prohibitively expensive beyond toy examples. This is especially problematic
when the Gaussian Kernel is used since the cost function becomes highly
non-convex. Given the original problem contained a orthogonality constraint,
the Gaussian Kernel further complicates the solution. Since the Gaussian
Kernel has historically superior performance, it is also the most important.
In light of these impediments, the idea of implementing a real time engine
with various kernels was simply not possible with the current method. The
proposed technique was originally introduced in a separate paper [10], and the
technique itself is called Dimensional Growth (DG). Although DG achieved its
objective of solving a complex non-convex problem, the original priority was
never on the actual implementation. As a result, DG lack the scalability
necessary as we enter into the era of big data.



One obvious solution is to separate out the components of the algorithm to
perform parallel processing. Parallel processing on the CPU was the initial
choice, but the speed improvement was still many magnitudes off. Moving the
algorithm onto the GPU was the next logical step. Techniques such as Frank
Wolfe [11], and ADMM [12] were researched and implemented. The constraint of
the cost function included a non-convex Stiefel Manifold. Due to the
non-convex cost function and constraint, Frank Wolfe is not a suitable
solution. The implementation of ADMM was successful even on a Stiefel
Manifold. Unfortunately, the power of ADMM requires the formulation to be
separable through dual decomposition. Yet, since the ADMM formulation was not
separable, ADMM without parallelization defeats its own purpose. As a matter
of fact, our implementation showed that ADMM converged significantly slower
than the original algorithm.

After much trial, it became clear that simply moving the algorithm onto the
GPU was not enough, the real change must be algorithmic. This is when we
researched into solutions that solves orthogonality constraints [13][16][17]
or reformulates the problem as a semi-definite programming (SDP) [14][15][18].
The reformulation of the problem into a SDP is not difficult even with a
orthogonality constraint. However, as the size of the solution space increase,
SDP algorithms are notoriously inflexible in terms of scalability.

More success came from the reimplementation of KDAC using orthogonality
constraint techniques. As we demonstrate in the experimental results, the
technique from [13] improved the convergence rate by many orders of magnitude.
The result was encouraging, but even with the success of this implementation,
the objective of approaching real time was still elusive. To further
exacerbate the problem, since KDAC is a non-convex problem, the algorithm must
be repeated up to 50 times to avoid being trapped in a local minimum.
Depending on the complexity of the data, it is difficult to determine the
number of random initializations.

Given a lack of other options, we set out to invent a new optimization
technique. From the list of requirements, the algorithm must be extremely fast
while scalable against the growth of data size. It must be able to handle
non-convex function constrained on a stiefel manifold. We wish to avoid
running many random initializations by discovering a theoretically reasonable
starting point. The algorithm itself should be easily understandable and
easily implementable through off the shelf tools. Since we wish to take
adventage of the GPU technology, the algorithm must be able to take adventage
of parallel processing. Lastly, we wish to provide a solid theoretical
foundation that motivates the procedures of the algorithm. To this end, we
have successfully designed an algorithm that met all these requirements. We
call this optimization technique the Iterative Spectral Method (ISM).



\end{document}
